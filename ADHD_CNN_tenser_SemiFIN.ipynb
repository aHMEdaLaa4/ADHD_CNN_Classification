{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f732c44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52c7327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ADHD_data_path='Dataset_proc/ADHD_full'\n",
    "Control_data_path='Dataset_proc/Control_full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77eae1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convertmat2mne(data):\n",
    "    ch_names =  ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2','F7','F8','T7','T8','P7','P8','Fz','Cz','Pz']\n",
    "    ch_types = ['eeg'] * 19\n",
    "    sampling_freq=128\n",
    "    info = mne.create_info(ch_names, ch_types=ch_types, sfreq=sampling_freq)\n",
    "    #info.set_montage('standard_1020')\n",
    "    data=mne.io.RawArray(data, info)\n",
    "    data.set_eeg_reference()\n",
    "    data.filter(l_freq=1,h_freq=35)\n",
    "    epochs=mne.make_fixed_length_epochs(data,duration=3.5,overlap=0)\n",
    "    return epochs.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c983cbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "ADHD_subject=[]\n",
    "for AD in glob(ADHD_data_path + '/*.mat'):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(AD, squeeze_me=True, struct_as_record=False)\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                data = mat_data[key]\n",
    "                data=np.transpose(data)\n",
    "                data=convertmat2mne(data)\n",
    "                ADHD_subject.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {AD}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d8a042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "Control_subject=[]\n",
    "for CO in glob(Control_data_path + '/*.mat'):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(CO, squeeze_me=True, struct_as_record=False)\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                data = mat_data[key]\n",
    "                data=np.transpose(data)\n",
    "                data=convertmat2mne(data)\n",
    "                Control_subject.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {CO}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a9e2b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ADHD_subject),len(Control_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69e6cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 60\n"
     ]
    }
   ],
   "source": [
    "ADHD_epochs_labels=[len(i)*[1] for i in ADHD_subject]\n",
    "Control_epochs_labels=[len(i)*[0] for i in Control_subject]\n",
    "print(len(ADHD_epochs_labels),len(Control_epochs_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20073eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 121 121\n"
     ]
    }
   ],
   "source": [
    "data_list=ADHD_subject+Control_subject\n",
    "label_list=ADHD_epochs_labels+Control_epochs_labels\n",
    "\n",
    "groups_list=[[i]*len(j) for i, j in enumerate(data_list)]\n",
    "'''\n",
    "Subject1:\n",
    "-epoch0\n",
    "-epoch1\n",
    "-epoch2\n",
    "Subject2:\n",
    "-epoch0\n",
    "-epoch1\n",
    "-epoch2\n",
    "_________\n",
    "Train\n",
    "Subject1:\n",
    "-epoch0\n",
    "                                            Testيبقى تبع ال Subjectتاني لنفس ال epoch و Tain معين يبقى تبع Subjectل epoch يحصل ان Split ممكن و انا بعمل\n",
    "                         و دي حاجة انا مش عاوزها        \n",
    "                                        \n",
    "                                                \n",
    "Test\n",
    "Subject1:\n",
    "-epoch1\n",
    "_________\n",
    "Train\n",
    "Subject1:\n",
    "-epoch0\n",
    "-epoch1\n",
    "-epoch2                               \n",
    "\n",
    "                                                          Test او Train تبقى تبع  Subjectلنفس ال epochsبحيث ان كل ال   Groupsل  DATAف انا قسمت ال \n",
    "Test\n",
    "Subject2:\n",
    "-epoch0\n",
    "-epoch1\n",
    "-epoch2\n",
    "\n",
    "\n",
    "'''\n",
    "print(len(data_list),len(label_list),len(groups_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd83c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold,LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "gkf=GroupKFold()\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#https://stackoverflow.com/questions/50125844/how-to-standard-scale-a-3d-matrix\n",
    "class StandardScaler3D(BaseEstimator,TransformerMixin):\n",
    "    #batch, sequence, channels\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.scaler.fit(X.reshape(-1, X.shape[2]))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return self.scaler.transform(X.reshape( -1,X.shape[2])).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "378fe005",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4770, 448, 19) (4770,) (4770,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_array=np.concatenate(data_list)\n",
    "label_array=np.concatenate(label_list)\n",
    "group_array=np.concatenate(groups_list)\n",
    "data_array=np.moveaxis(data_array,1,2)\n",
    "\n",
    "print(data_array.shape,label_array.shape,group_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158efd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "#for train_index, val_index in gkf.split(data_array, label_array, groups=group_array):\n",
    "    #train_features,train_labels=data_array[train_index],label_array[train_index]\n",
    "    #val_features,val_labels=data_array[val_index],label_array[val_index]\n",
    "    #scaler=StandardScaler3D()\n",
    "    #train_features=scaler.fit_transform(train_features)\n",
    "    #val_features=scaler.transform(val_features)\n",
    "    #break\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(data_array, label_array, groups=group_array)):\n",
    "    train_features, val_features = data_array[train_index], data_array[val_index]\n",
    "    train_labels, val_labels = label_array[train_index], label_array[val_index]\n",
    "    scaler=StandardScaler3D()\n",
    "    train_features=scaler.fit_transform(train_features)\n",
    "    val_features=scaler.transform(val_features)\n",
    "    break\n",
    "    '''\n",
    "# Save X_train\n",
    "np.save('train_features.npy', train_features)\n",
    "# Save y_train\n",
    "np.save('train_labels.npy', train_labels)\n",
    "# Save X_test\n",
    "np.save('val_features.npy', val_features)\n",
    "# Save y_test\n",
    "np.save('val_labels.npy', val_labels)    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742cf534-13a7-4c6d-9c21-99e2e8a2282c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In subsequent runs\n",
    "train_features = np.load('train_features.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "val_features = np.load('val_features.npy')\n",
    "val_labels = np.load('val_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bebf16d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3816,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf91d8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,concatenate,Flatten,GRU,Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "#resource:https://github.com/dll-ncai/eeg_pre-diagnostic_screening/blob/master/code/chrononet/chrono.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "006f87a7-b307-4a67-bc3c-02819b8ea3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 448, 64)           3712      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 448, 64)           256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 448, 64)           12352     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 448, 64)           256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 224, 64)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 224, 128)          24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 224, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 224, 128)          49280     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 224, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 112, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91713 (358.25 KB)\n",
      "Trainable params: 90945 (355.25 KB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D,BatchNormalization,LeakyReLU,MaxPool1D,\\\n",
    "GlobalAveragePooling1D,Dense,Dropout,AveragePooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.backend import clear_session\n",
    "def cnnmodel():\n",
    "    clear_session()\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Block 1\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=(448, 19)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=cnnmodel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db0c0a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "96/96 [==============================] - 15s 122ms/step - loss: 0.4194 - accuracy: 0.7993 - val_loss: 0.5120 - val_accuracy: 0.7998\n",
      "Epoch 2/25\n",
      "96/96 [==============================] - 10s 105ms/step - loss: 0.2496 - accuracy: 0.8931 - val_loss: 0.2827 - val_accuracy: 0.8973\n",
      "Epoch 3/25\n",
      "96/96 [==============================] - 10s 107ms/step - loss: 0.1960 - accuracy: 0.9248 - val_loss: 0.2430 - val_accuracy: 0.9036\n",
      "Epoch 4/25\n",
      "96/96 [==============================] - 10s 102ms/step - loss: 0.1314 - accuracy: 0.9476 - val_loss: 0.2143 - val_accuracy: 0.9182\n",
      "Epoch 5/25\n",
      "96/96 [==============================] - 10s 104ms/step - loss: 0.1391 - accuracy: 0.9447 - val_loss: 0.2026 - val_accuracy: 0.9361\n",
      "Epoch 6/25\n",
      "96/96 [==============================] - 10s 104ms/step - loss: 0.0990 - accuracy: 0.9612 - val_loss: 0.1312 - val_accuracy: 0.9486\n",
      "Epoch 7/25\n",
      "96/96 [==============================] - 9s 99ms/step - loss: 0.0841 - accuracy: 0.9683 - val_loss: 0.2691 - val_accuracy: 0.8983\n",
      "Epoch 8/25\n",
      "96/96 [==============================] - 11s 116ms/step - loss: 0.0668 - accuracy: 0.9772 - val_loss: 0.2115 - val_accuracy: 0.9193\n",
      "Epoch 9/25\n",
      "96/96 [==============================] - 10s 101ms/step - loss: 0.0540 - accuracy: 0.9798 - val_loss: 0.1506 - val_accuracy: 0.9444\n",
      "Epoch 10/25\n",
      "96/96 [==============================] - 10s 108ms/step - loss: 0.0720 - accuracy: 0.9733 - val_loss: 0.1745 - val_accuracy: 0.9361\n",
      "Epoch 11/25\n",
      "96/96 [==============================] - 10s 103ms/step - loss: 0.0486 - accuracy: 0.9856 - val_loss: 0.1130 - val_accuracy: 0.9539\n",
      "Epoch 12/25\n",
      "96/96 [==============================] - 9s 98ms/step - loss: 0.0451 - accuracy: 0.9843 - val_loss: 0.1837 - val_accuracy: 0.9423\n",
      "Epoch 13/25\n",
      "96/96 [==============================] - 9s 97ms/step - loss: 0.0399 - accuracy: 0.9864 - val_loss: 0.1564 - val_accuracy: 0.9476\n",
      "Epoch 14/25\n",
      "96/96 [==============================] - 10s 103ms/step - loss: 0.0470 - accuracy: 0.9806 - val_loss: 0.1873 - val_accuracy: 0.9371\n",
      "Epoch 15/25\n",
      "96/96 [==============================] - 9s 97ms/step - loss: 0.0295 - accuracy: 0.9914 - val_loss: 0.1631 - val_accuracy: 0.9518\n",
      "Epoch 16/25\n",
      "96/96 [==============================] - 9s 97ms/step - loss: 0.0435 - accuracy: 0.9843 - val_loss: 0.0894 - val_accuracy: 0.9717\n",
      "Epoch 17/25\n",
      "96/96 [==============================] - 9s 98ms/step - loss: 0.0352 - accuracy: 0.9858 - val_loss: 0.1746 - val_accuracy: 0.9350\n",
      "Epoch 18/25\n",
      "96/96 [==============================] - 9s 96ms/step - loss: 0.0259 - accuracy: 0.9898 - val_loss: 0.0878 - val_accuracy: 0.9748\n",
      "Epoch 19/25\n",
      "96/96 [==============================] - 9s 97ms/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 0.0816 - val_accuracy: 0.9633\n",
      "Epoch 20/25\n",
      "96/96 [==============================] - 9s 96ms/step - loss: 0.0159 - accuracy: 0.9953 - val_loss: 0.1345 - val_accuracy: 0.9602\n",
      "Epoch 21/25\n",
      "96/96 [==============================] - 9s 98ms/step - loss: 0.0322 - accuracy: 0.9890 - val_loss: 0.1030 - val_accuracy: 0.9654\n",
      "Epoch 22/25\n",
      "96/96 [==============================] - 9s 96ms/step - loss: 0.0365 - accuracy: 0.9866 - val_loss: 0.1601 - val_accuracy: 0.9549\n",
      "Epoch 23/25\n",
      "96/96 [==============================] - 9s 96ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.0898 - val_accuracy: 0.9706\n",
      "Epoch 24/25\n",
      "96/96 [==============================] - 9s 97ms/step - loss: 0.0213 - accuracy: 0.9921 - val_loss: 0.1510 - val_accuracy: 0.9518\n",
      "Epoch 25/25\n",
      "96/96 [==============================] - 9s 96ms/step - loss: 0.0223 - accuracy: 0.9924 - val_loss: 0.1255 - val_accuracy: 0.9706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21680ab7750>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=cnnmodel()\n",
    "model.fit(train_features,train_labels,epochs=25,batch_size=40,validation_data=(val_features,val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7396a1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 27ms/step - loss: 0.1255 - accuracy: 0.9706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12549765408039093, 0.9706498980522156]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_features,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55fe3671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "289e3eb8-7320-4bf1-accf-1655414d3d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on val_labely: 97.06%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert the list of predictions to a numpy array\n",
    "predictions_array = np.squeeze(np.array(predictions))\n",
    "\n",
    "# Round the predictions to get the predicted class (0 or 1)\n",
    "rounded_predictions = np.round(predictions_array).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(val_labels, rounded_predictions)\n",
    "print(f\"Accuracy on val_labely: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42b9f19d-7953-4682-99b7-62aaa91917f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[391  21]\n",
      " [  7 535]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9706498951781971"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(val_labels, rounded_predictions)\n",
    "print(cm)\n",
    "accuracy_score(val_labels, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d880ef7-6827-47aa-a646-bfe79a088441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
